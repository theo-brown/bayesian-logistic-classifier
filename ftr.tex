\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[justification=centering]{caption}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[outputdir=out]{minted}
\usepackage{subcaption}


\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{Equation}


\begin{document}

    \title{Bayesian Logistic Classification}
    \author{3F8: Inference Coursework \\ Theo Brown \\ Selwyn College, University of Cambridge}
    \date{\today}
    \maketitle

    \begin{abstract}
        Abstract goes here
    \end{abstract}
    
    \section{Introduction}\label{sec:introduction}

    \section{Laplace approximation for logistic regression}
    The Laplace approximation finds a Gaussian, $q(\bm{x})$, that provides a good approximation to a probability distribution $p(\bm{x})$ near a local maximum of $p(\bm{x})$.
    This is useful when the desired distribution $p(\bm{x})$ is intractable and difficult to manipulate.

    In the previous report, the maximum-likelihood estimate for the model weights was used in classification\footnote{For the model definition, and definitions of $\bm{X}$, $\bm{y}$, $\tilde{\bm{x}}$, etc, see the previous report.}.
    A full posterior distribution of the weights is required to perform fully Bayesian classification:
    \begin{align}
        p(\bm{w} | \bm{X}, \bm{y}) = \frac{p(\bm{X} | \bm{w}, \bm{y}) p(\bm w)}{p(\bm{X} | \bm{y})}
        \label{eq:posterior}
    \end{align}
    Calculating the normalising constant (also called the evidence) $p(\bm{X} | \bm{y})$ requires integrating a product of logistic functions, which is intractable.
    The Laplace approximation can be used to find an approximate posterior distribution and an approximate predictive distribution for the logistic classifier, while avoiding the difficult integral.
    The derivation of these results is found in \autoref{app:derivation}.

    In this application, the prior distribution of the model weights is chosen to be Gaussian:
    \begin{align}
        p(\bm{w}) = \mathcal{N}(\bm{w}; 0, \bm{I}\sigma_0^2)
    \end{align}
    The Laplace approximation requires the location of a maximum in the distribution to be approximated, which in this case will be the MAP estimate of $\bm{w}$.
    This can be found by applying gradient ascent to the posterior.
    Firstly, the gradient of the log-posterior is calculated, using the expression for the derivative of the log-likelihood from the previous report:
    \begin{align}
        \frac{\partial}{\partial \bm{w}} \log p(\bm{w} | \tilde{\bm{X}}, \bm{y}) &= \frac{\partial}{\partial \bm{w}} \log p(\tilde{\bm{X}} | \bm{w} , \bm{y}) + \frac{\partial}{\partial \bm{w}} \log p(\bm{w}) \nonumber \\
        &= \tilde{\bm{X}} (\textbf{y} - \sigma(\tilde{\bm{X}}^T \bm{w})) + \frac{1}{\sigma_0^2} \bm{w}
    \end{align}
    This can then be used with a standard gradient-based solver such as \verb`scipy.optimize.fmin_l_bfgs_b` to find a value for $\bm{w}_\text{MAP}$.

    The Laplace approximation of the posterior of the model weights is:
    \begin{align}
        p(\bm{w} | \bm{X}, \bm{y}) \approx \mathcal{N}(\bm{w}; \bm{w}_\text{MAP}, \bm{C}_N)
    \end{align}
    And the predictive distribution is:
    \begin{align}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \sigma\left(\frac{\mu_p}{\sqrt{1 + \sigma_p^2\lambda^2}}\right)
    \end{align}
    where
    \begin{align}
        \bm{C}_N &= \left(\frac{1}{\sigma_0^2}\bm{I} + \tilde{\bm{X}}\tilde{\bm{X}}^T \bm{\sigma} (1 - \bm{\sigma})\right)^{-1} \\
        \bm{\sigma} &= \sigma(\tilde{\bm{X}}^T \bm{w}) \\
        \mu_p &= {\bm{x}^*}^T \bm{w}_\text{MAP} \\
        \sigma_p^2 &= {\bm{x}^*}^T \bm{C}_N \bm{x^*} \\
        \lambda^2 &= \frac{\pi}{8}
    \end{align}

       \newpage
    \appendix
    \section{Deriving Bayesian Logistic Classification using the Laplace Approximation}
    \label{app:derivation}

    \subsection{Laplace approximation}\label{sec:laplace-approximation}

    To define a probability distribution function (PDF) $p(\bm{x})$, $\bm{x} \in \mathbb{R}^N$ it is necessary to integrate a function to find the normalising constant K:
    \begin{align}
        p(\bm{x}) := \frac{f(\bm{x})}{\int f(\bm{x}) d\bm{x}} = \frac{1}{K} f(\bm{x})
    \end{align}
    In many cases, the integral $\int f(\bm{x}) d\bm{x}$ is intractable or does not have a closed-form solution.

    The Laplace approximation finds a Gaussian, $q(\bm{x})$, that provides a good approximation to $p(\bm{x})$ near a local maximum of $p(\bm{x})$.
    As $q(\bm{x})$ is Gaussian, its normalising constant is easy to find, so the problem of solving $\int f(\bm{x}) d\bm{x}$ is avoided.
    To find $q(\bm{x})$, we start with the truncated Taylor expansion of $\ln f(\bm{x})$ around a local maximum $\bm{x}_0$:
    \begin{align}
        \nonumber
        \ln f(\bm{x}) &\approx \ln f(\bm{x}_0) + \nabla \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0)
        + \frac{1}{2} (\bm{x} - \bm{x}_0)^T \nabla^2 \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0)
    \end{align}
    At a maximum of $f(\bm{x})$, $\nabla \ln f(\bm{x}) = 0$ as the logarithm is a monotonic function.
    Hence, close to $\bm{x}_0$:
    \begin{align}
        \ln f(\bm{x}) \approx \ln f(\bm{x}_0) + \frac{1}{2} (\bm{x} - \bm{x}_0)^T \nabla^2 \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0) \nonumber \\
        f(\bm{x}) \approx f(\bm{x}_0) \exp \left(\frac{1}{2} (\bm{x} - \bm{x}_0)^T \nabla^2 \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0) \right)
        \label{eq:laplace_f_gaussian}
    \end{align}
    \autoref{eq:laplace_f_gaussian} is of the form of an un-normalised Gaussian. Let $\bm{P} = -\nabla^2 \ln f(\bm{x})\big|_{\bm{x} = \bm{x}_0}$,
    and normalise \autoref{eq:laplace_f_gaussian} to get an approximation for $p(\bm{x})$:
    \begin{align}
        p(\bm{x}) &\approx \frac{1}{(2\pi)^\frac{M}{2} \det\bm{P}^{-\frac{1}{2}}} \exp \left(\frac{1}{2} (\bm{x} - \bm{x}_0)^T \bm{P} (\bm{x} - \bm{x}_0) \right) \nonumber \\
        &= \mathcal{N}(\bm{x}; \bm{x}_0, \bm{P}^{-1})
        \label{eq:laplace_approximation}
    \end{align}
    For this to hold, $\bm{P}$ must be positive definite, i.e. $\bm{x}_0$ must be a maximum.

    This method can be used instead to find an approximate value of the normalising constant K. Substituting
    \autoref{eq:laplace_f_gaussian} into the definition of K:
    \begin{align}
        K = \int f(\bm{x}) d\bm{x} &\approx f(\bm{x}_0) \int \exp \left(\frac{1}{2} (\bm{x} - \bm{x}_0)^T \bm{P} (\bm{x} - \bm{x}_0) \right) d\bm{x} \nonumber \\
        &= \frac{(2\pi)^\frac{N}{2}}{\det \bm{P}^\frac{1}{2}} f(\bm{x}_0)
        \label{eq:normalising_constant}
    \end{align}

    \subsection{Bayesian logistic regression}
    \subsubsection{Posterior distribution}
    Given that the posterior Laplace approximation will be Gaussian, it is sensible to choose a Gaussian prior for the model weights:
    \begin{align}
        p(\bm{w}) = \mathcal{N}(\bm{w}; \bm{m}_0, \bm{C}_0)
        \label{eq:gaussian_prior}
    \end{align}
    In order to apply the Laplace approximation, we need the location of a maximum in the posterior.
    Using the results for the likelihood of the logistic model in the previous report and the log of \autoref{eq:gaussian_prior}, the log-posterior of the model weights is:
    \begin{align}
        \ln p(\bm{w} | \bm{X}, \bm{y}) =
        & \sum_{n=1}^{N} y_n \log\sigma( \bm{w}^T \tilde{\bm{x}}_n) + (1-y_n) \log\sigma(-\bm{w}^T \tilde{\bm{x}}_n) \nonumber \\
        & + \frac{1}{2} (\bm{w} - \bm{w}_0)^T \bm{C}_0^{-1} (\bm{w} - \bm{w}_0) \nonumber \\
        & + \text{const}
        \label{eq:log_posterior}
    \end{align}
    The value of $\bm{w}$ at the maximum, $\bm{w}_{\text{MAP}}$, can be found by setting the derivative of
    \autoref{eq:log_posterior} to zero.
    The mean of $q(\bm{w})$ will be set to $\bm{w}_{\text{MAP}}$.

    Using \autoref{eq:laplace_approximation}, the covariance matrix of $q(\bm{w})$ can be found:
    \begin{align}
        \bm{C}_N^{-1} &= -\nabla^2 \ln p(\bm{w} | \bm{X}, \bm{y}) \big|_{\bm{w} =\bm{w}_{\text{MAP}}} \nonumber \\
        &= \bm{C}_0^{-1}
        + \sum_{n=1}^N \sigma(\bm{w}^T \tilde{\bm{x}}_n) \sigma(-\bm{w}^T \tilde{\bm{x}}_n)\tilde{\bm{x}}_n\tilde{\bm{x}}_n^T
    \end{align}
    Defining $\bm{\sigma} = \sigma(\tilde{\bm{X}}^T \bm{w})$, this can be written in vector form as:
    \begin{align}
         \bm{C}_N^{-1} &= \bm{C}_0^{-1} + \tilde{\bm{X}}\tilde{\bm{X}}^T \bm{\sigma} (1 - \bm{\sigma})
    \end{align}
    Hence, using \autoref{eq:laplace_approximation}, the posterior distribution is:
    \begin{align}
        p(\bm{w} | \bm{X}, \bm{y}) \approx \mathcal{N}(\bm{w}; \bm{w}_\text{MAP}, \bm{C}_N)
    \end{align}
    And using \autoref{eq:normalising_constant}, the normalizing constant is:
    \begin{align}
        p(\bm{X} | \bm{y}) = \frac{(2\pi)^\frac{N}{2}}{\det \bm{C}_N^{-\frac{1}{2}}}
                            p(\bm{X} | \bm{w} = \bm{w}_\text{MAP}, \bm{y}) p(\bm{w} = \bm{w}_\text{MAP})
    \end{align}

    \subsection{Predictive distribution}
    The predictive distribution can also be approximated using the Laplace method:
    \begin{align}
        p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &= \int p(y^* = 1 | \bm{x}^*, \bm{w}) p(\bm{w} | \bm{y}, \bm{X}) d\bm{w} \nonumber \\
        & \approx \int \sigma(\bm{w}^T\bm{x}^*) q(\bm{w}) d\bm{w}
    \end{align}
    Using the sifting property of the delta function:
    \begin{align}
        \sigma(\bm{w}^T \bm{x}) = \int \delta(a - \bm{w}^T \bm{x}) \sigma(a) da
    \end{align}
    Hence:
     \begin{align}
        p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \int \int \delta(a - \bm{w}^T \bm{x}^*) \sigma(a) q(\bm{w}) d\bm{w} da \nonumber \\
         &= \int \sigma(a) \int \delta(a - {\bm{x}^*}^T \bm{w}) q(\bm{w}) d\bm{w} da \nonumber
    \end{align}
    The inner integral applies a linear constraint to $q(\bm{w})$, as the argument of the delta function is 0 unless $a = {\bm{x}^*}^T \bm{w}$.
    Hence, the approximate predictive distribution is:
    \begin{align}
        \label{eq:approx_predictive}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \int \sigma(a) \mathcal{N}(a; {\bm{x}^*}^T \bm{w}_\text{MAP}, {\bm{x}^*}^T \bm{C}_N \bm{x^*}) da \nonumber \\
        &= \int \sigma(a) \mathcal{N}(a; \mu_p, \sigma_p^2) da
     \end{align}
    Where
    \begin{align}
        \mu_p &= {\bm{x}^*}^T \bm{w}_\text{MAP}\\
        \sigma_p^2 &= {\bm{x}^*}^T \bm{C}_N \bm{x^*}
    \end{align}
    This integral cannot be expressed analytically, so another approximation is required.
    The logistic function can be approximated well by a probit function scaled such that the gradient of the two functions at the origin are equal.
    It can be shown that this gives:
    \begin{align}
        \label{eq:probit_approx}
        \sigma(x) \approx \Phi^{-1}\left(\sqrt{\frac{\pi}{8}} x\right) = \Phi^{-1}(\lambda x)
    \end{align}
    Substituting into \autoref{eq:approx_predictive} and evaluating using properties of the probit function gives:
    \begin{align}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \int \Phi^{-1}(\lambda x) \mathcal{N}(a; \mu_p, \sigma_p) da \nonumber \\
        &= \Phi^{-1}\left(\frac{\mu_p}{\sqrt{\lambda^{-2} + \sigma_p^2}}\right)
     \end{align}
    Using \autoref{eq:probit_approx}, this can be converted back into a logistic function:
    \begin{align}
        \label{eq:predictive_distribution}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \sigma\left(\frac{\mu_p}{\sqrt{1 + \sigma_p^2\lambda^2}}\right)
    \end{align}
\end{document}
