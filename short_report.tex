\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[outputdir=out]{minted}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{pdfpages}
\usepackage[justification=centering]{caption}
\usepackage{algpseudocode}

\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{Equation}


\begin{document}

    \title{Logistic Classification}
    \author{3F8 laboratory experiment \\ Theo Brown \\ Selwyn College, University of Cambridge}
    \date{\today}
    \maketitle

    \begin{abstract}
        Enter a short summary here.
    \end{abstract}

    \section{Introduction}\label{sec:introduction}
    Classification is the task of grouping data points according to their shared features, and assigning each group
    a label. This is often a trivial manual task for small datasets, but with very large collections of data it
    becomes very time intensive, so the development of good machine classifiers is an important area in statistical
    learning. Trained classifiers can be used as predictive tools to identify the most probable class that a new
    input will belong to, which could, for example, be useful in a medical setting, such as identifying the most
    at-risk patients.

    A key tool in classification is the ability to classify non-linear datasets, where the boundary between classes
    is not a straight line. This report explores the use of a set of non-linear basis functions (in this case,
    radial functions), which are applied to the inputs before classifying, to allow the discovery of non-linear
    decision boundaries.

    \section{Derivation of the gradient of the log-likelihood}\label{sec:gradient-derivation}
    If each datapoint's class label is assumed to be independent and identically generated from a Bernoulli distribution
    in which the probability of a class label is given by a logistic function applied to weighted inputs, the
    log-likelihood is as follows:
    \begin{align*}
        \mathcal{L}(w) = \log P(y|X, w) &= \log \prod_{n=1}^{N} \sigma(w^T \tilde{x}^{(n)})^{y^{(n)}}\sigma(-w^T \tilde{x}^{(n)})^{1-y^{(n)}} \\
        &= \sum_{n=1}^{N} y^{(n)} \log\sigma(w^T \tilde{x}^{(n)}) + (1-y^{(n)}) \log\sigma(-w^T \tilde{x}^{(n)})
    \end{align*}
    The derivative of the log-likelihood is calculated using the identities $\frac{d\sigma(x)}{dx} = \sigma(x)\sigma(-x)$
    and $\frac{d\log(x)}{dx} = \frac{1}{x}$. Applying the chain rule:
    \begin{align*}
        \frac{\partial \mathcal{L}}{\partial w} &= \sum_{n=1}^{N} y^{(n)} \sigma(-w^T \tilde{x}^{(n)}) \tilde{x}^{(n)} - (1-y^{(n)}) \sigma(w^T \tilde{x}^{(n)})  \tilde{x}^{(n)}
        \\ &= \sum_{n=1}^{N} \left(y^{(n)} - \sigma(w^T \tilde{x}^{(n)}) \right)  \tilde{x}^{(n)}
    \end{align*}

    \section{Gradient ascent algorithm}\label{sec:gradient-ascent-algorithm}
    To find the parameter $w$ that maximises the log-likelihood, a gradient ascent algorithm is used. Define
    $\tilde{X}$ and y as in \autoref{sec:gradient-derivation}, then let n be the number of iterations to run the
    algorithm for and r be the learning rate. The algorithm is defined as follows:

    \begin{algorithmic}
        \Procedure{GradientAscent}{$\tilde{X}$, y, n, r}
            \State $w \gets [0 \dots 0]$ \Comment{Initialise the weights}
            \For{$i\gets 1, n$}
                \State $s \gets \sigma(\tilde{X}w)$
                \State $d \gets \tilde{X} (y - \sigma)$ \Comment{Calculate the gradient}
                \State $w \gets w + rd $ \Comment{Move along the gradient, scaled by learning rate}
            \EndFor
        \EndProcedure
    \end{algorithmic}

    Note that the weights do not necessarily need to be initialised as zeros - they could have been selected randomly, or
    initialised as ones, for example. Provided the number of iterations is large enough and the algorithm does not get
    stuck in a local maximum, the effect of the initial values should be negligible.

    The learning rate, n, is chosen empirically. Starting with a value chosen such the algorithm shows signs
    of convergence, the learning rate is increased until the algorithm begins to oscillate. The rate is then decreased
    slightly so that the oscillation is avoided. Using this method, a fast rate of convergence is ensured and divergence
    avoided.

    In Python\footnote{The Python 3.5+ matrix multiplication operator $@$ provides an easy method of vectorising the
    operations}, the gradient ascent algorithm can be implemented as follows:

    \begin{minted}{python}
        import numpy as np

        def logistic(x):
            return 1 / (1 + np.exp(-x))

        def gradient_ascent(X, y,
                            number_iterations, learning_rate):
            # Prepend a column of ones to the input data (X)
            X1 = np.column_stack((np.ones(X.shape[0]), X))
            # Initialise the weights
            w = np.ones(X1.shape[1])

            for i in range(number_iterations):
                # Calculate the gradient
                dL = (y - logistic(X1 @ w)) @ X1
                # Move in the direction of the gradient,
                # scaled by learning rate
                w += learning_rate * dL
            return w
    \end{minted}

    \section{Data visualisation}\label{sec:data-visualisation}
    The data is visualised in the two-dimensional input space in \autoref{fig:data}. From the plot it is clear that a
    linear classifier will perform badly on the data, as the classes are not linearly separable: there is no straight
    boundary that successfully distinguishes between red and blue data points.

    \begin{figure}[h]
        \label{fig:data}
        \centering
        \includegraphics[width=0.8\textwidth]{plots/data.png}
        \caption{Plot of the two-dimensional input features (1000 datapoints, 2 classes)}
    \end{figure}

    \section{Linear classifier training}\label{sec:linear-training}
    The data was split into a training set of 800 points and a test set of 200 points, and the gradient descent
    algorithm with a learning rate of 0.001 was applied to find the weights for the linear classifier. The mean
    log-likelihood (where the mean is taken across all of the weights) is plotted in \autoref{fig:linear_log_likelihood} for
    the training and test datasets at each iteration.

    \begin{figure}[h]
        \label{fig:linear_log_likelihood}
        \centering
        \includegraphics[width=0.8\textwidth]{plots/log_likelihood.png}
        \caption{Plot of the mean log-likelihood of the weights for the test and training data, over the
        course of model training}
    \end{figure}

    \autoref{fig:linear_log_likelihood} shows that a plateau is reached for the log-likelihood of the parameters in both the
    training and test sets, after which no further improvements can be made. This suggests an optimum has been found for
    this model. After training there remains a difference between the performance on the test and training sets, but not
    at a significant level (see \autoref{sec:linear-performance}).

    \section{Linear classifier performance}\label{sec:linear-performance}
    The predictive distribution of the model is shown in \autoref{fig:linear_predictive_distribution}. It is clear that
    this model fails to accurately classify the two classes. It identifies that the general principle that more of the
    points with $x_2 < 0$ are class 1 and more of the points with $x_2 > 0$ values are class 2, but it is apparent to
    the human eye that this is overly simplified and fails to identify the `islands' where one class appears surrounded
    by the other.

    \begin{figure}[h]
        \label{fig:linear_predictive_distribution}
        \centering
        \includegraphics[width=0.8\textwidth]{plots/predictive_distribution_linear.png}
        \caption{Model predictive distribution overlaid on input datapoints coloured by class}
    \end{figure}

    The mean log-likelihood of the trained model evaluated on the training and test sets is shown in
    \autoref{tab:performance_metrics_linear}. It shows a difference of 3-4\% between the training and test data, which is
    sufficiently small to suggest that overfitting has been avoided. As expected, the performance is marginally better
    for the training set, as this was the data that the model was fit to.

    To find the predicted class label for a point $\boldsymbol{x}_n$, a hard threshold is applied to the predicted class
    probabilities:
    \begin{align*}
        \hat{y}_n = \begin{cases}
                        0 & \sigma(\tilde{\boldsymbol{x}}_n^T \boldsymbol{w}) \leq 0.5 \\
                        1 & \sigma(\tilde{\boldsymbol{x}}_n^T \boldsymbol{w}) > 0.5
                    \end{cases}
    \end{align*}

    Using the predicted class labels, the confusion matrix for the test data is found
    (\autoref{tab:performance_metrics_linear}). The confusion matrix is a good method of assessing the classifier's
    performance, as it shows the fraction of correct and incorrect classifications for each class.

    \autoref{tab:performance_metrics_linear} shows that the linear classifier successfully identifies class 0 about
    75\% of the time. Comparing with \autoref{fig:linear_predictive_distribution}, we can see that the decision boundary
    (shown as a contour of value 0.0) correctly separates roughly four out of six visually identifiable clusters of
    class 0, and fails to account for two. Similarly, the classifier identifies class 1 70\% of the time, and the
    decision boundary separates approximately four out of five visually identifiable class 1 clusters.

    \begin{figure}[h]
        \label{tab:performance_metrics_linear}
        \centering
        \begin{tabular}{c|c}
            \textbf{Performance metric} & \textbf{Value} \\
            \hline
            Mean log-likelihood, training data & -0.629 \\
            Mean log-likelihood, test data & -0.604 \\
            $\begin{pmatrix}
                 P(\hat{y}=0 | y=0) & P(\hat{y}=1 | y=0) \\
                 P(\hat{y}=0 | y=1) & P(\hat{y}=1 | y=1) \\
            \end{pmatrix}$ &
            $\begin{pmatrix}
                0.764 & 0.235 \\
                0.276 & 0.723 \\
            \end{pmatrix}$ \\
        \end{tabular}
        \caption{Performance metrics for the linear classifier}
    \end{figure}

    We conclude that the linear classifier does not provide a particularly useful result for this dataset.
    A new method is presented in the next section, which uses a non-linear basis function to allow the classifier to
    identify non-linear class boundaries.


\end{document}