\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[justification=centering]{caption}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[outputdir=out]{minted}
\usepackage{subcaption}

\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{Equation}


\begin{document}

    \title{Bayesian Logistic Classification}
    \author{3F8: Inference Coursework \\ Theo Brown \\ Selwyn College, University of Cambridge}
    \date{\today}
    \maketitle

    \begin{abstract}
        Abstract goes here
    \end{abstract}
    
    \section{Introduction}\label{sec:introduction}

    \section{Laplace approximation for logistic regression}
    \subsection{Justification}
    In the previous report, the maximum-likelihood estimate for the model weights was used in classification\footnote{For the model definition, and definitions of $\bm{X}$, $\bm{y}$, $\tilde{\bm{x}}$, etc, see the previous report.}.
    A full posterior distribution of the weights is required to perform fully Bayesian classification:
    \begin{align}
        p(\bm{w} | \bm{X}, \bm{y}) = \frac{p(\bm{X} | \bm{w}, \bm{y}) p(\bm w)}{p(\bm{X} | \bm{y})}
        \label{eq:posterior}
    \end{align}
    Calculating the model evidence $p(\bm{X} | \bm{y})$ requires integrating a product of logistic functions, which is intractable.
    The Laplace approximation can be used to find an approximate posterior distribution and an approximate predictive distribution for the logistic classifier, while avoiding the difficult integral.
    The derivation of these results is found in \autoref{app:derivation}.

    In this application, the prior distribution of the model weights is chosen to be Gaussian:
    \begin{align}
        p(\bm{w}) = \mathcal{N}(\bm{w}; 0, \bm{I}\sigma_0^2)
    \end{align}
    
    \subsubsection{Finding the MAP estimate}
    The Laplace approximation requires the location of a maximum in the distribution to be approximated, which in this case will be the MAP estimate of $\bm{w}$.
    This can be found by applying gradient ascent to the posterior.
    Firstly, the gradient of the log-posterior is calculated:
    \begin{align}
        \frac{\partial}{\partial \bm{w}} \log p(\bm{w} | \tilde{\bm{X}}, \bm{y}) &= \frac{\partial}{\partial \bm{w}} \log p(\tilde{\bm{X}} | \bm{w} , \bm{y}) + \frac{\partial}{\partial \bm{w}} \log p(\bm{w}) \nonumber \\
        &= \tilde{\bm{X}} (\textbf{y} - \sigma(\tilde{\bm{X}}^T \bm{w})) + \frac{1}{\sigma_0^2} \bm{w}
    \end{align}
    This can then be used with a standard gradient-based solver to find a value for $\bm{w}_\text{MAP}$.
    \subsubsection{Results of the Laplace approximation}
    Applying the Laplace approximation gives following results:

    \begin{itemize}
        \item Approximate posterior distribution of $\bm{w}$:
        \begin{align}
            p(\bm{w} | \bm{X}, \bm{y}) &\approx \mathcal{N}(\bm{w}; \bm{w}_\text{MAP}, \bm{S}_N)
        \end{align}
        \noindent
        \item Approximate predictive distribution for new points $\bm{x}_*$:
        \begin{align}
            p(y_* = 1 | \bm{x}_*, \bm{y}, \bm{X}) &\approx \sigma\left(\frac{\mu_p}{\sqrt{1 + \sigma_p^2\lambda^2}}\right)
        \end{align}
        \item Approximate model evidence:
        \begin{align}
            p(\bm{X} | \bm{y}) = \sqrt{\frac{(2\pi)^N}{\det \bm{S}_N^{-1}}}
                                 \exp \left(\mathcal{L}(\bm{w}_\text{MAP}) + \mathcal{P}(\bm{w}_\text{MAP})\right)
        \end{align}
    \end{itemize}
    where
    \begin{align}
        \bm{S}_N &= \left(\frac{1}{\sigma_0^2}\bm{I} + \tilde{\bm{X}}\tilde{\bm{X}}^T \bm{\sigma} (1 - \bm{\sigma})\right)^{-1} \\
        \bm{\sigma} &= \sigma(\tilde{\bm{X}}^T \bm{w}) \\
        \mu_p &= {\tilde{\bm{x}}_*}^T \bm{w}_\text{MAP} \\
        \sigma_p^2 &= {\tilde{\bm{x}}_*}^T \bm{S}_N \tilde{\bm{x}}_* \\
        \lambda^2 &= \frac{\pi}{8} \\
        \mathcal{L}(\bm{w}_\text{MAP}) &= \log p(\bm{X} | \bm{w}, \bm{y}) \big|_{\bm{w} = \bm{w}_\text{MAP}} \\
        \mathcal{P}(\bm{w}_\text{MAP}) &= \log p(\bm{w}) \big|_{\bm{w} = \bm{w}_\text{MAP}}
    \end{align}
    and $N$ is the number of data points $\bm{x}_n$.

    \subsection{Implementation in Python}
    Firstly, the data is loaded and split into training and test sets.
    \begin{minted}{python}
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.utils import shuffle

    X_data, y_data = shuffle(np.loadtxt('data/X.txt'),
                             np.loadtxt('data/y.txt'))

    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, train_size=800)
    \end{minted}
    Functions are defined to expand the data through a set of radial basis functions centred on the training points, and prepend a column of ones:
    \begin{minted}{python}
    def prepend_ones(M):
        return np.column_stack((np.ones(M.shape[0]), M))

    def expand_rbf(l, X, Z=X_train):
        X2 = np.sum(X**2, 1)
        Z2 = np.sum(Z**2, 1)
        ones_Z = np.ones(Z.shape[ 0 ])
        ones_X = np.ones(X.shape[ 0 ])
        r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)
        return prepend_ones(np.exp(-0.5 / l**2 * r2))
    \end{minted}
    A numerical approximation of the MAP estimate for the weights is found:
    \begin{minted}{python}
    from scipy.optimize import fmin_l_bfgs_b as minimise

    def logistic(x):
        return 1 / (1 + np.exp(-x))

    def log_prior(w, variance):
        return -1 / (2 * variance) * (w.T @ w)

    def log_likelihood(w, X, y):
        sigma = logistic(X @ w)
        return np.sum(y * np.log(sigma)
                      + (1 - y) * np.log(1 - sigma))

    def negative_log_posterior(w, X, y, prior_variance):
        return -(log_likelihood(w, X, y) + log_prior(w, prior_variance))

    def negative_posterior_gradient(w, X, y, prior_variance):
        return -((y - logistic(X @ w)) @ X - w / prior_variance)

    def find_w_map(X, y, w0=None, prior_variance=1):
        if w0 is None:
            w0 = np.random.normal(size=X.shape[1])
        w_map, posterior_at_wmap, d = minimise(negative_log_posterior,
                                               w0,
                                               negative_posterior_gradient,
                                               args=[X, y, prior_variance])
        return w_map

    expanded_training_set = expand_rbf(rbf_width, X_train)
    w_map = find_w_map(expanded_training_set, y_train)
    \end{minted}

    Note that \verb`scipy.optimize.fmin_l_bfgs_b` is a minimisation function, so to maximise the posterior we have to work with the negative posterior and negative posterior gradient.
    Once $\bm{w}_{\text{MAP}}$ is found, the Laplace approximation for the model evidence and the predictive distribution can be found:
    \begin{minted}{python}
    def S_N_inv(w, rbf_width, prior_variance):
        M = w.shape[0]
        X_tilde = expand_rbf(rbf_width, X_train)
        sigma = logistic(X_tilde @ w)
        return np.identity(M) / prior_variance + X_tilde @ X_tilde.T @ sigma @ (1 - sigma)

    def log_evidence(w, X, y, rbf_width, prior_variance):
        d = np.linalg.det(S_N_inv(w, rbf_width, prior_variance))
        return (M/2)*np.log(2*np.pi) - 0.5*np.log(d) + log_likelihood(w, X, y) + log_prior(w, prior_variance)

    def laplace_prediction(inputs, weights, rbf_width, prior_variance):
        X_tilde = expand_rbf(rbf_width, X_train)
        sigma = logistic(X_tilde @ weights)
        S_N = np.linalg.inv(S_N_inv(weights, rbf_width, prior_variance))
        predictive_mean = inputs @ weights
        predictive_variance = np.array([x.T @ C_N @ x for x in inputs])
        return logistic(predictive_mean / np.sqrt(1 + predictive_variance*np.pi/8))
    \end{minted}

    \section{Performance of the Laplace Approximation}



    \newpage
    \appendix
    \section{Deriving Bayesian Logistic Classification using the Laplace Approximation}
    \label{app:derivation}

    \subsection{Laplace approximation}\label{sec:laplace-approximation}

    To define a probability distribution function (PDF) $p(\bm{x})$, $\bm{x} \in \mathbb{R}^N$ it is necessary to integrate a function to find the normalising constant K:
    \begin{align}
        p(\bm{x}) := \frac{f(\bm{x})}{\int f(\bm{x}) d\bm{x}} = \frac{1}{K} f(\bm{x})
    \end{align}
    In many cases, the integral $\int f(\bm{x}) d\bm{x}$ is intractable or does not have a closed-form solution.

    The Laplace approximation finds a Gaussian, $q(\bm{x})$, that provides a good approximation to $p(\bm{x})$ near a local maximum of $p(\bm{x})$.
    As $q(\bm{x})$ is Gaussian, its normalising constant is easy to find, so the problem of solving $\int f(\bm{x}) d\bm{x}$ is avoided.
    To find $q(\bm{x})$, we start with the truncated Taylor expansion of $\ln f(\bm{x})$ around a local maximum $\bm{x}_0$:
    \begin{align}
        \nonumber
        \ln f(\bm{x}) &\approx \ln f(\bm{x}_0) + \nabla \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0)
        + \frac{1}{2} (\bm{x} - \bm{x}_0)^T \nabla^2 \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0)
    \end{align}
    At a maximum of $f(\bm{x})$, $\nabla \ln f(\bm{x}) = 0$ as the logarithm is a monotonic function.
    Hence, close to $\bm{x}_0$:
    \begin{align}
        \ln f(\bm{x}) \approx \ln f(\bm{x}_0) + \frac{1}{2} (\bm{x} - \bm{x}_0)^T \nabla^2 \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0) \nonumber \\
        f(\bm{x}) \approx f(\bm{x}_0) \exp \left(\frac{1}{2} (\bm{x} - \bm{x}_0)^T \nabla^2 \ln f(\bm{x}) \big|_{\bm{x} = \bm{x}_0} (\bm{x} - \bm{x}_0) \right)
        \label{eq:laplace_f_gaussian}
    \end{align}
    \autoref{eq:laplace_f_gaussian} is of the form of an un-normalised Gaussian. Let $\bm{P} = -\nabla^2 \ln f(\bm{x})\big|_{\bm{x} = \bm{x}_0}$,
    and normalise \autoref{eq:laplace_f_gaussian} to get an approximation for $p(\bm{x})$:
    \begin{align}
        p(\bm{x}) &\approx \frac{1}{(2\pi)^\frac{M}{2} \det\bm{P}^{-\frac{1}{2}}} \exp \left(\frac{1}{2} (\bm{x} - \bm{x}_0)^T \bm{P} (\bm{x} - \bm{x}_0) \right) \nonumber \\
        &= \mathcal{N}(\bm{x}; \bm{x}_0, \bm{P}^{-1})
        \label{eq:laplace_approximation}
    \end{align}
    For this to hold, $\bm{P}$ must be positive definite, i.e. $\bm{x}_0$ must be a maximum.

    This method can be used instead to find an approximate value of the normalising constant K. Substituting
    \autoref{eq:laplace_f_gaussian} into the definition of K:
    \begin{align}
        K = \int f(\bm{x}) d\bm{x} &\approx f(\bm{x}_0) \int \exp \left(\frac{1}{2} (\bm{x} - \bm{x}_0)^T \bm{P} (\bm{x} - \bm{x}_0) \right) d\bm{x} \nonumber \\
        &= \frac{(2\pi)^\frac{N}{2}}{\det \bm{P}^\frac{1}{2}} f(\bm{x}_0)
        \label{eq:normalising_constant}
    \end{align}

    \subsection{Bayesian logistic regression}
    \subsubsection{Posterior distribution}
    Given that the posterior Laplace approximation will be Gaussian, a Gaussian prior for the model weights is chosen:
    \begin{align}
        p(\bm{w}) = \mathcal{N}(\bm{w}; \bm{m}_0, \bm{S}_0)
        \label{eq:gaussian_prior}
    \end{align}
    In order to apply the Laplace approximation to the posterior, the location of a maximum in the posterior is required
    (this will be the MAP estimate).
    Using the results for the likelihood of the logistic model in the previous report and the log of \autoref{eq:gaussian_prior},
    the log-posterior of the model weights is:
    \begin{align}
        \ln p(\bm{w} | \bm{X}, \bm{y}) =
        & \sum_{n=1}^{N} y_n \log\sigma( \bm{w}^T \tilde{\bm{x}}_n) + (1-y_n) \log\sigma(-\bm{w}^T \tilde{\bm{x}}_n) \nonumber \\
        & + \frac{1}{2} (\bm{w} - \bm{w}_0)^T \bm{S}_0^{-1} (\bm{w} - \bm{w}_0) \nonumber \\
        & + \text{const}
        \label{eq:log_posterior}
    \end{align}
    The value of $\bm{w}$ at the maximum, $\bm{w}_{\text{MAP}}$, can be found by setting the derivative of
    \autoref{eq:log_posterior} to zero.
    The mean of $q(\bm{w})$ will be set to $\bm{w}_{\text{MAP}}$.

    Using \autoref{eq:laplace_approximation}, the covariance matrix of $q(\bm{w})$ can be found:
    \begin{align}
        \bm{S}_N^{-1} &= -\nabla^2 \ln p(\bm{w} | \bm{X}, \bm{y}) \big|_{\bm{w} =\bm{w}_{\text{MAP}}} \nonumber \\
        &= \bm{S}_0^{-1}
        + \sum_{n=1}^N \sigma(\bm{w}^T \tilde{\bm{x}}_n) \sigma(-\bm{w}^T \tilde{\bm{x}}_n)\tilde{\bm{x}}_n\tilde{\bm{x}}_n^T
    \end{align}
    Defining $\bm{\sigma} = \sigma(\tilde{\bm{X}}^T \bm{w})$, this can be written in vector form as:
    \begin{align}
         \bm{S}_N^{-1} &= \bm{S}_0^{-1} + \tilde{\bm{X}}\tilde{\bm{X}}^T \bm{\sigma} (1 - \bm{\sigma})
    \end{align}
    Hence, using \autoref{eq:laplace_approximation}, the posterior distribution is:
    \begin{align}
        p(\bm{w} | \bm{X}, \bm{y}) \approx \mathcal{N}(\bm{w}; \bm{w}_\text{MAP}, \bm{S}_N)
    \end{align}
    And using \autoref{eq:normalising_constant}, the normalizing constant is:
    \begin{align}
        p(\bm{X} | \bm{y}) = \frac{(2\pi)^\frac{N}{2}}{\det \bm{S}_N^{-\frac{1}{2}}}
                            p(\bm{X} | \bm{w} = \bm{w}_\text{MAP}, \bm{y}) p(\bm{w} = \bm{w}_\text{MAP})
    \end{align}

    \subsection{Predictive distribution}
    The predictive distribution can also be approximated using the Laplace method:
    \begin{align}
        p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &= \int p(y^* = 1 | \bm{x}^*, \bm{w}) p(\bm{w} | \bm{y}, \bm{X}) d\bm{w} \nonumber \\
        & \approx \int \sigma(\bm{w}^T\bm{x}^*) q(\bm{w}) d\bm{w}
    \end{align}
    Using the sifting property of the delta function:
    \begin{align}
        \sigma(\bm{w}^T \bm{x}) = \int \delta(a - \bm{w}^T \bm{x}) \sigma(a) da
    \end{align}
    Hence:
     \begin{align}
        p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \int \int \delta(a - \bm{w}^T \bm{x}^*) \sigma(a) q(\bm{w}) d\bm{w} da \nonumber \\
         &= \int \sigma(a) \int \delta(a - {\bm{x}^*}^T \bm{w}) q(\bm{w}) d\bm{w} da \nonumber
    \end{align}
    The inner integral applies a linear constraint to $q(\bm{w})$, as the argument of the delta function is 0 unless $a = {\bm{x}^*}^T \bm{w}$.
    Hence, the approximate predictive distribution is:
    \begin{align}
        \label{eq:approx_predictive}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \int \sigma(a) \mathcal{N}(a; {\bm{x}^*}^T \bm{w}_\text{MAP}, {\bm{x}^*}^T \bm{S}_N \bm{x^*}) da \nonumber \\
        &= \int \sigma(a) \mathcal{N}(a; \mu_p, \sigma_p^2) da
     \end{align}
    Where
    \begin{align}
        \mu_p &= {\bm{x}^*}^T \bm{w}_\text{MAP}\\
        \sigma_p^2 &= {\bm{x}^*}^T \bm{S}_N \bm{x^*}
    \end{align}
    This integral cannot be expressed analytically, so another approximation is required.
    The logistic function can be approximated well by a probit function scaled such that the gradient of the two functions at the origin are equal.
    It can be shown that this gives:
    \begin{align}
        \label{eq:probit_approx}
        \sigma(x) \approx \Phi^{-1}\left(\sqrt{\frac{\pi}{8}} x\right) = \Phi^{-1}(\lambda x)
    \end{align}
    Substituting into \autoref{eq:approx_predictive} and evaluating using properties of the probit function gives:
    \begin{align}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \int \Phi^{-1}(\lambda x) \mathcal{N}(a; \mu_p, \sigma_p) da \nonumber \\
        &= \Phi^{-1}\left(\frac{\mu_p}{\sqrt{\lambda^{-2} + \sigma_p^2}}\right)
     \end{align}
    Using \autoref{eq:probit_approx}, this can be converted back into a logistic function:
    \begin{align}
        \label{eq:predictive_distribution}
         p(y^* = 1 | \bm{x}^*, \bm{y}, \bm{X}) &\approx \sigma\left(\frac{\mu_p}{\sqrt{1 + \sigma_p^2\lambda^2}}\right)
    \end{align}
\end{document}
